{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jib/SSD/universlaDocs/MyGPT/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "\n",
    "from gpt_tokenizers import BytePairEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_vocab_size = 7500\n",
    "byte_pair_encoder = BytePairEncoder(bpe_vocab_size, 2)\n",
    "byte_pair_encoder.load(\"encoder_directory/encoder-vocab.json\", \"encoder_directory/encoder-merges.txt\")#{args.model_dir}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code that this should replace for ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset_in_chunks(encoder, dataset, chunk_size=1000000):\n",
    "    encoded_chunks = []\n",
    "    start_idx = 0\n",
    "    while start_idx < len(dataset):\n",
    "        end_idx = min(start_idx + chunk_size, len(dataset))\n",
    "        chunk = dataset[start_idx:end_idx]\n",
    "        encoded_chunk = encoder.encode(chunk)\n",
    "        encoded_chunk_tensor = torch.tensor(encoded_chunk, dtype=torch.long)  # Convert list to tensor\n",
    "        encoded_chunks.append(encoded_chunk_tensor)\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return torch.cat(encoded_chunks)  # Concatenate tensors before returning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(data, block_size, batch_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    # device = data[0].device\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ChunkyText Dataset\n",
    "Loads up a dataset with text chunks so we can process faster with map, and take advantages of the features that the dataset object offers. \n",
    "\n",
    "It integrates with [Apache Arrow](https://arrow.apache.org/overview/) ([more info](https://huggingface.co/docs/datasets/about_arrow)) to give these advantages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class chunkyText(Dataset):\n",
    "    def __init__(self, file, chunk_size):\n",
    "        # self._data: Table = _check_table(arrow_table)\n",
    "        self.chunks = []\n",
    "        with open(file, encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            start_idx = 0\n",
    "            while start_idx < len(text):\n",
    "                end_idx = min(start_idx + chunk_size, len(text))\n",
    "                chunk = text[start_idx:end_idx]\n",
    "                self.chunks.append(chunk)\n",
    "                start_idx = end_idx\n",
    "\n",
    "        # sets all of the needed vars underthehood for more complex dataset features\n",
    "        chunks = pa.array(self.chunks, type=pa.string())\n",
    "        arrow_table = pa.table([chunks], names=[\"chunks\"])\n",
    "        Dataset.__init__(self, arrow_table) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)  # this wont ever update\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.last:\n",
    "            self.last = next(self.mapped_iter)\n",
    "\n",
    "        nextval = next(self.mapped_iter)\n",
    "        val = (self.last, nextval)\n",
    "        self.last = next\n",
    "\n",
    "        return (val)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        return(self.chunks[idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to load the dataset\n",
    "\n",
    "Use dataset text to load the files line by line and each row it just a line of string text. Its nice, but ever string has a diff len.\n",
    "```python\n",
    "load_dataset(\"text\", data_files={\"train\": [\"my_text_1.txt\", \"my_text_2.txt\"], \"test\": \"my_test_file.txt\"})\n",
    "```\n",
    "You can also use the datadir arg to specify a dir of text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000000\n",
    "dataset = chunkyText(\"input_data_files/cleaned_orca_dataset.txt\", chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1631"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the dataset into tokens\n",
    "Using the map method for dataset lets you use multiprocessing to process the dataset in batches\n",
    "```python\n",
    "dataset_with_duplicates = dataset.map(lambda batch: {\"b\": batch[\"a\"] * 2}, remove_columns=[\"a\"], batched=True, batch_size=1000, num_proc=10)\n",
    "```\n",
    "The num proc is what specifies how many thread to use.\n",
    "There is also a with_rank=True parameter for multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1631, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocess import set_start_method\n",
    "set_start_method(\"fork\")  # this is needed for linux maybe windows too spawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextInputSequence must be str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m byte_pair_encoder\u001b[39m.\u001b[39;49mencode([\u001b[39m\"\u001b[39;49m\u001b[39mthis is a test\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlyl, not string typehahah\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/SSD/universlaDocs/MyGPT/gpt_tokenizers.py:60\u001b[0m, in \u001b[0;36mBytePairEncoder.encode\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, s: \u001b[39mstr\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mencode(s)\u001b[39m.\u001b[39mids\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded\n",
      "File \u001b[0;32m~/SSD/universlaDocs/MyGPT/venv/lib/python3.11/site-packages/tokenizers/implementations/base_tokenizer.py:215\u001b[0m, in \u001b[0;36mBaseTokenizer.encode\u001b[0;34m(self, sequence, pair, is_pretokenized, add_special_tokens)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m sequence \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mencode: `sequence` can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be `None`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 215\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode(sequence, pair, is_pretokenized, add_special_tokens)\n",
      "\u001b[0;31mTypeError\u001b[0m: TextInputSequence must be str"
     ]
    }
   ],
   "source": [
    "byte_pair_encoder.encode([\"this is a test\", \"lyl, not string typehahah\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3888,\n",
       " 1442,\n",
       " 316,\n",
       " 4777,\n",
       " 256,\n",
       " 3286,\n",
       " 259,\n",
       " 1901,\n",
       " 289,\n",
       " 256,\n",
       " 2289,\n",
       " 1958,\n",
       " 11,\n",
       " 758,\n",
       " 509,\n",
       " 293,\n",
       " 79,\n",
       " 305,\n",
       " 289,\n",
       " 264,\n",
       " 2289,\n",
       " 13,\n",
       " 198,\n",
       " 5288,\n",
       " 2289,\n",
       " 323,\n",
       " 626,\n",
       " 3182,\n",
       " 264,\n",
       " 4061,\n",
       " 4895,\n",
       " 2639,\n",
       " 678,\n",
       " 266,\n",
       " 425,\n",
       " 335,\n",
       " 576,\n",
       " 264,\n",
       " 2639,\n",
       " 678,\n",
       " 276,\n",
       " 2799,\n",
       " 281,\n",
       " 617,\n",
       " 366,\n",
       " 1256,\n",
       " 281,\n",
       " 4829,\n",
       " 341,\n",
       " 941,\n",
       " 81,\n",
       " 730,\n",
       " 2966,\n",
       " 220,\n",
       " 7,\n",
       " 49,\n",
       " 35,\n",
       " 37,\n",
       " 8,\n",
       " 574,\n",
       " 608,\n",
       " 931,\n",
       " 289,\n",
       " 264,\n",
       " 2775,\n",
       " 220,\n",
       " 7,\n",
       " 82,\n",
       " 1151,\n",
       " 1189,\n",
       " 11,\n",
       " 5237,\n",
       " 5424,\n",
       " 4119,\n",
       " 8,\n",
       " 13,\n",
       " 296,\n",
       " 2799,\n",
       " 35,\n",
       " 37,\n",
       " 574,\n",
       " 608,\n",
       " 931,\n",
       " 2142,\n",
       " 885,\n",
       " 812,\n",
       " 316,\n",
       " 2063,\n",
       " 327,\n",
       " 264,\n",
       " 574,\n",
       " 608,\n",
       " 931,\n",
       " 1970,\n",
       " 319,\n",
       " 2543,\n",
       " 6538,\n",
       " 264,\n",
       " 5286,\n",
       " 266,\n",
       " 551,\n",
       " 76,\n",
       " 375,\n",
       " 1492,\n",
       " 289,\n",
       " 264,\n",
       " 293,\n",
       " 79,\n",
       " 305,\n",
       " 2639,\n",
       " 678,\n",
       " 13,\n",
       " 296,\n",
       " 293,\n",
       " 79,\n",
       " 305,\n",
       " 323,\n",
       " 256,\n",
       " 2639,\n",
       " 678,\n",
       " 266,\n",
       " 264,\n",
       " 416,\n",
       " 79,\n",
       " 305,\n",
       " 323,\n",
       " 256,\n",
       " 1097,\n",
       " 289,\n",
       " 574,\n",
       " 608,\n",
       " 931,\n",
       " 289,\n",
       " 264,\n",
       " 2775,\n",
       " 220,\n",
       " 58,\n",
       " 82,\n",
       " 1151,\n",
       " 1189,\n",
       " 11,\n",
       " 5237,\n",
       " 5424,\n",
       " 11,\n",
       " 4119,\n",
       " 60,\n",
       " 327,\n",
       " 6538,\n",
       " 264,\n",
       " 3727,\n",
       " 1440,\n",
       " 293,\n",
       " 264,\n",
       " 2639,\n",
       " 678,\n",
       " 13,\n",
       " 1387,\n",
       " 256,\n",
       " 2639,\n",
       " 678,\n",
       " 345,\n",
       " 782,\n",
       " 1367,\n",
       " 4806,\n",
       " 2799,\n",
       " 35,\n",
       " 37,\n",
       " 574,\n",
       " 608,\n",
       " 302,\n",
       " 3305,\n",
       " 11,\n",
       " 264,\n",
       " 416,\n",
       " 79,\n",
       " 305,\n",
       " 812,\n",
       " 600,\n",
       " 369,\n",
       " 459,\n",
       " 289,\n",
       " 833,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 32,\n",
       " 37,\n",
       " 34,\n",
       " 373,\n",
       " 73,\n",
       " 1070,\n",
       " 220,\n",
       " 7,\n",
       " 354,\n",
       " 418,\n",
       " 319,\n",
       " 82,\n",
       " 8,\n",
       " 6,\n",
       " 82,\n",
       " 1051,\n",
       " 323,\n",
       " 364,\n",
       " 1356,\n",
       " 79,\n",
       " 399,\n",
       " 1256,\n",
       " 68,\n",
       " 1921,\n",
       " 4355,\n",
       " 297,\n",
       " 308,\n",
       " 856,\n",
       " 373,\n",
       " 73,\n",
       " 1070,\n",
       " 2466,\n",
       " 3070,\n",
       " 373,\n",
       " 66,\n",
       " 505,\n",
       " 76,\n",
       " 88,\n",
       " 997,\n",
       " 569,\n",
       " 13,\n",
       " 198,\n",
       " 46,\n",
       " 305,\n",
       " 79,\n",
       " 305,\n",
       " 25,\n",
       " 198,\n",
       " 58,\n",
       " 198,\n",
       " 220,\n",
       " 220,\n",
       " 58,\n",
       " 1,\n",
       " 32,\n",
       " 37,\n",
       " 34,\n",
       " 373,\n",
       " 73,\n",
       " 1070,\n",
       " 220,\n",
       " 7,\n",
       " 354,\n",
       " 418,\n",
       " 319,\n",
       " 82,\n",
       " 8,\n",
       " 1,\n",
       " 11,\n",
       " 1983,\n",
       " 71,\n",
       " 280,\n",
       " 1051,\n",
       " 1,\n",
       " 11,\n",
       " 1983,\n",
       " 50,\n",
       " 1356,\n",
       " 79,\n",
       " 399,\n",
       " 1256,\n",
       " 68,\n",
       " 1921,\n",
       " 4355,\n",
       " 297,\n",
       " 308,\n",
       " 1,\n",
       " 60,\n",
       " 11,\n",
       " 198,\n",
       " 220,\n",
       " 220,\n",
       " 58,\n",
       " 1,\n",
       " 32,\n",
       " 73,\n",
       " 1070,\n",
       " 2466,\n",
       " 3070,\n",
       " 373,\n",
       " 66,\n",
       " 505,\n",
       " 76,\n",
       " 88,\n",
       " 1,\n",
       " 11,\n",
       " 1983,\n",
       " 608,\n",
       " 2152,\n",
       " 390,\n",
       " 1,\n",
       " 11,\n",
       " 1983,\n",
       " 50,\n",
       " 1356,\n",
       " 79,\n",
       " 399,\n",
       " 1256,\n",
       " 68,\n",
       " 1921,\n",
       " 4355,\n",
       " 297,\n",
       " 308,\n",
       " 1,\n",
       " 60,\n",
       " 198,\n",
       " 60,\n",
       " 198,\n",
       " 198,\n",
       " 38,\n",
       " 278,\n",
       " 273,\n",
       " 418,\n",
       " 353,\n",
       " 1048,\n",
       " 87,\n",
       " 5176,\n",
       " 270,\n",
       " 346,\n",
       " 1161,\n",
       " 278,\n",
       " 12,\n",
       " 1557,\n",
       " 67,\n",
       " 2639,\n",
       " 678,\n",
       " 327,\n",
       " 664,\n",
       " 66,\n",
       " 3565,\n",
       " 281,\n",
       " 459,\n",
       " 529,\n",
       " 5337,\n",
       " 25,\n",
       " 777,\n",
       " 2025,\n",
       " 414,\n",
       " 1274,\n",
       " 383,\n",
       " 966,\n",
       " 3414,\n",
       " 51,\n",
       " 88,\n",
       " 3250,\n",
       " 921,\n",
       " 26,\n",
       " 777,\n",
       " 2025,\n",
       " 414,\n",
       " 1274,\n",
       " 383,\n",
       " 966,\n",
       " 1090,\n",
       " 4847,\n",
       " 26,\n",
       " 777,\n",
       " 2025,\n",
       " 414,\n",
       " 1274,\n",
       " 383,\n",
       " 966,\n",
       " 7256,\n",
       " 49,\n",
       " 649,\n",
       " 1809,\n",
       " 418,\n",
       " 26,\n",
       " 777,\n",
       " 2025,\n",
       " 414,\n",
       " 1274,\n",
       " 383,\n",
       " 966,\n",
       " 1827,\n",
       " 273,\n",
       " 326,\n",
       " 644,\n",
       " 5044,\n",
       " 416,\n",
       " 289,\n",
       " 4607,\n",
       " 26,\n",
       " 777,\n",
       " 2025,\n",
       " 414,\n",
       " 1274,\n",
       " 383,\n",
       " 966,\n",
       " 1181,\n",
       " 4408,\n",
       " 1406,\n",
       " 283,\n",
       " 5341,\n",
       " 198,\n",
       " 44,\n",
       " 2025,\n",
       " 414,\n",
       " 1274,\n",
       " 383,\n",
       " 966,\n",
       " 323,\n",
       " 256,\n",
       " 1809,\n",
       " 2543,\n",
       " 769,\n",
       " 1391,\n",
       " 4847,\n",
       " 921,\n",
       " 334,\n",
       " 256,\n",
       " 5044,\n",
       " 14,\n",
       " 20,\n",
       " 1827,\n",
       " 273,\n",
       " 326,\n",
       " 644,\n",
       " 11,\n",
       " 1083,\n",
       " 885,\n",
       " 1181,\n",
       " 4408,\n",
       " 1406,\n",
       " 283,\n",
       " 5341,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 6778,\n",
       " 1187,\n",
       " 1623,\n",
       " 1222,\n",
       " 293,\n",
       " 529,\n",
       " 2035,\n",
       " 543,\n",
       " 1332,\n",
       " 71,\n",
       " 30,\n",
       " 198,\n",
       " 198,\n",
       " 5042,\n",
       " 758,\n",
       " 326,\n",
       " 3402,\n",
       " 256,\n",
       " 535,\n",
       " 317,\n",
       " 330,\n",
       " 256,\n",
       " 6981,\n",
       " 2251,\n",
       " 758,\n",
       " 3915,\n",
       " 337,\n",
       " 2122,\n",
       " 256,\n",
       " 282,\n",
       " 1937,\n",
       " 400,\n",
       " 266,\n",
       " 7260,\n",
       " 298,\n",
       " 2115,\n",
       " 472,\n",
       " 337,\n",
       " 13,\n",
       " 410,\n",
       " 758,\n",
       " 5186,\n",
       " 389,\n",
       " 256,\n",
       " 3906,\n",
       " 289,\n",
       " 256,\n",
       " 1550,\n",
       " 266,\n",
       " 758,\n",
       " 5484,\n",
       " 2314,\n",
       " 6305,\n",
       " 2025,\n",
       " 514,\n",
       " 256,\n",
       " 5556,\n",
       " 13,\n",
       " 768,\n",
       " 198,\n",
       " 34,\n",
       " 71,\n",
       " 343,\n",
       " 342,\n",
       " 2012,\n",
       " 5378,\n",
       " 450,\n",
       " 25,\n",
       " 373,\n",
       " 13,\n",
       " 670,\n",
       " 699,\n",
       " 4978,\n",
       " 4805,\n",
       " 266,\n",
       " 349,\n",
       " 893,\n",
       " 389,\n",
       " 264,\n",
       " 1550,\n",
       " 293,\n",
       " 256,\n",
       " 6795,\n",
       " 273,\n",
       " 13,\n",
       " 1406,\n",
       " 13,\n",
       " 282,\n",
       " 259,\n",
       " 2376,\n",
       " 264,\n",
       " 298,\n",
       " 2115,\n",
       " 276,\n",
       " 4470,\n",
       " 256,\n",
       " 269,\n",
       " 441,\n",
       " 555,\n",
       " 83,\n",
       " 1161,\n",
       " 11,\n",
       " 266,\n",
       " 758,\n",
       " 2835,\n",
       " 962,\n",
       " 13,\n",
       " 988,\n",
       " 13,\n",
       " 758,\n",
       " 286,\n",
       " 1892,\n",
       " 264,\n",
       " 535,\n",
       " 317,\n",
       " 293,\n",
       " 293,\n",
       " 74,\n",
       " 266,\n",
       " 3182,\n",
       " 264,\n",
       " 282,\n",
       " 1937,\n",
       " 400,\n",
       " 276,\n",
       " 1962,\n",
       " 256,\n",
       " 4516,\n",
       " 330,\n",
       " 552,\n",
       " 4376,\n",
       " 11,\n",
       " 6647,\n",
       " 3115,\n",
       " 337,\n",
       " 757,\n",
       " 334,\n",
       " 256,\n",
       " 326,\n",
       " 543,\n",
       " 293,\n",
       " 264,\n",
       " 916,\n",
       " 13,\n",
       " 1256,\n",
       " 13,\n",
       " 5439,\n",
       " 276,\n",
       " 4470,\n",
       " 552,\n",
       " 1027,\n",
       " 266,\n",
       " 6372,\n",
       " 337,\n",
       " 2314,\n",
       " 1931,\n",
       " 755,\n",
       " 918,\n",
       " 268,\n",
       " 264,\n",
       " 7093,\n",
       " 289,\n",
       " 337,\n",
       " 276,\n",
       " 1573,\n",
       " 264,\n",
       " 3746,\n",
       " 3563,\n",
       " 768,\n",
       " 345,\n",
       " 5219,\n",
       " 13,\n",
       " 198,\n",
       " 34,\n",
       " 13,\n",
       " 410,\n",
       " 758,\n",
       " 286,\n",
       " 1892,\n",
       " 264,\n",
       " 535,\n",
       " 317,\n",
       " 293,\n",
       " 293,\n",
       " 74,\n",
       " 266,\n",
       " 3182,\n",
       " 264,\n",
       " 282,\n",
       " 1937,\n",
       " 400,\n",
       " 276,\n",
       " 1962,\n",
       " 256,\n",
       " 4516,\n",
       " 330,\n",
       " 552,\n",
       " 4376,\n",
       " 11,\n",
       " 6647,\n",
       " 3115,\n",
       " 337,\n",
       " 757,\n",
       " 334,\n",
       " 256,\n",
       " 326,\n",
       " 543,\n",
       " 293,\n",
       " 264,\n",
       " 916,\n",
       " 13,\n",
       " 818,\n",
       " 529,\n",
       " 3332,\n",
       " 341,\n",
       " 11,\n",
       " 768,\n",
       " 323,\n",
       " 928,\n",
       " 5792,\n",
       " 264,\n",
       " 4537,\n",
       " 289,\n",
       " 3182,\n",
       " 264,\n",
       " 535,\n",
       " 317,\n",
       " 11,\n",
       " 282,\n",
       " 1937,\n",
       " 400,\n",
       " 11,\n",
       " 266,\n",
       " 298,\n",
       " 2115,\n",
       " 11,\n",
       " 2470,\n",
       " 323,\n",
       " 1410,\n",
       " 843,\n",
       " 885,\n",
       " 276,\n",
       " 790,\n",
       " 768,\n",
       " 311,\n",
       " 2834,\n",
       " 293,\n",
       " 264,\n",
       " 2054,\n",
       " 6565,\n",
       " 2639,\n",
       " 678,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 47,\n",
       " 5800,\n",
       " 5378,\n",
       " 264,\n",
       " 2533,\n",
       " 6379,\n",
       " 25,\n",
       " 271,\n",
       " 616,\n",
       " 276,\n",
       " 3153,\n",
       " 264,\n",
       " 3987,\n",
       " 289,\n",
       " 1906,\n",
       " 276,\n",
       " 702,\n",
       " 256,\n",
       " 7180,\n",
       " 266,\n",
       " 5378,\n",
       " 5782,\n",
       " 626,\n",
       " 337,\n",
       " 13,\n",
       " 988,\n",
       " 696,\n",
       " 679,\n",
       " 282,\n",
       " 5800,\n",
       " 1069,\n",
       " 389,\n",
       " 334,\n",
       " 256,\n",
       " 1017,\n",
       " 6379,\n",
       " 312,\n",
       " 264,\n",
       " 7180,\n",
       " 1983,\n",
       " 1283,\n",
       " 4806,\n",
       " 24,\n",
       " 15,\n",
       " 16,\n",
       " 11,\n",
       " 264,\n",
       " 941,\n",
       " 275,\n",
       " 273,\n",
       " 448,\n",
       " 289,\n",
       " 373,\n",
       " 371,\n",
       " 5568,\n",
       " 1525,\n",
       " 311,\n",
       " 264,\n",
       " 4537,\n",
       " 531,\n",
       " 2470,\n",
       " 264,\n",
       " 261,\n",
       " 2034,\n",
       " 7219,\n",
       " 1406,\n",
       " 81,\n",
       " 279,\n",
       " 944,\n",
       " 2592,\n",
       " 12,\n",
       " 70,\n",
       " 1771,\n",
       " 434,\n",
       " 761,\n",
       " 291,\n",
       " 481,\n",
       " 289,\n",
       " 1805,\n",
       " 3739,\n",
       " 364,\n",
       " 3070,\n",
       " 466,\n",
       " 6346,\n",
       " 11,\n",
       " 220,\n",
       " 48,\n",
       " 806,\n",
       " 1623,\n",
       " 4652,\n",
       " 11,\n",
       " 364,\n",
       " 3070,\n",
       " 373,\n",
       " 371,\n",
       " 5568,\n",
       " 1525,\n",
       " 11,\n",
       " 892,\n",
       " 280,\n",
       " 3041,\n",
       " 1525,\n",
       " 11,\n",
       " 4441,\n",
       " 1316,\n",
       " 277,\n",
       " 1525,\n",
       " 266,\n",
       " 466,\n",
       " 281,\n",
       " 3301,\n",
       " 373,\n",
       " 371,\n",
       " 5568,\n",
       " 1525,\n",
       " 312,\n",
       " 695,\n",
       " 724,\n",
       " 309,\n",
       " 448,\n",
       " 13,\n",
       " 409,\n",
       " 3307,\n",
       " 264,\n",
       " 4197,\n",
       " 289,\n",
       " 6726,\n",
       " 570,\n",
       " 327,\n",
       " 802,\n",
       " 393,\n",
       " 1595,\n",
       " 275,\n",
       " 357,\n",
       " 7219,\n",
       " 761,\n",
       " 291,\n",
       " 481,\n",
       " 388,\n",
       " 997,\n",
       " 849,\n",
       " 657,\n",
       " 256,\n",
       " 270,\n",
       " 275,\n",
       " 273,\n",
       " 295,\n",
       " 6726,\n",
       " 570,\n",
       " 327,\n",
       " 311,\n",
       " 2326,\n",
       " 317,\n",
       " 312,\n",
       " 4177,\n",
       " 1197,\n",
       " 273,\n",
       " 434,\n",
       " 264,\n",
       " 3104,\n",
       " 309,\n",
       " 448,\n",
       " 13,\n",
       " 1387,\n",
       " 264,\n",
       " 988,\n",
       " 291,\n",
       " 308,\n",
       " 279,\n",
       " 4106,\n",
       " 289,\n",
       " 373,\n",
       " 371,\n",
       " 5568,\n",
       " 1525,\n",
       " 1150,\n",
       " 514,\n",
       " 2337,\n",
       " 11,\n",
       " 264,\n",
       " 761,\n",
       " 291,\n",
       " 481,\n",
       " 3723,\n",
       " 4515,\n",
       " 2850,\n",
       " 310,\n",
       " 1676,\n",
       " 289,\n",
       " 264,\n",
       " 988,\n",
       " 297,\n",
       " 5361,\n",
       " 1254,\n",
       " 7153,\n",
       " 289,\n",
       " 373,\n",
       " 371,\n",
       " 5568,\n",
       " 1525,\n",
       " 2891,\n",
       " 30,\n",
       " 198,\n",
       " 32,\n",
       " 77,\n",
       " 3257,\n",
       " 273,\n",
       " 25,\n",
       " 198,\n",
       " 33,\n",
       " 5453,\n",
       " 330,\n",
       " 264,\n",
       " 7180,\n",
       " 11,\n",
       " 3010,\n",
       " 264,\n",
       " 7255,\n",
       " 831,\n",
       " 2739,\n",
       " 972,\n",
       " 266,\n",
       " 416,\n",
       " 1178,\n",
       " 281,\n",
       " 289,\n",
       " 264,\n",
       " 4806,\n",
       " 24,\n",
       " 15,\n",
       " 16,\n",
       " 941,\n",
       " 275,\n",
       " 273,\n",
       " 448,\n",
       " 289,\n",
       " 373,\n",
       " 371,\n",
       " 5568,\n",
       " 1525,\n",
       " 11,\n",
       " 5602,\n",
       " 264,\n",
       " 326,\n",
       " 499,\n",
       " 281,\n",
       " 266,\n",
       " 5253,\n",
       " 289,\n",
       " 264,\n",
       " 270,\n",
       " 275,\n",
       " 273,\n",
       " 295,\n",
       " 6726,\n",
       " 570,\n",
       " 11,\n",
       " 357,\n",
       " 1637,\n",
       " 357,\n",
       " 264,\n",
       " 1164,\n",
       " 6726,\n",
       " 570,\n",
       " 295,\n",
       " 6140,\n",
       " 289,\n",
       " 264,\n",
       " 4528,\n",
       " 310,\n",
       " 1676,\n",
       " 1228,\n",
       " 5960,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 41,\n",
       " 1324,\n",
       " 3060,\n",
       " 256,\n",
       " 2678,\n",
       " 1573,\n",
       " 266,\n",
       " 584,\n",
       " 372,\n",
       " 4607,\n",
       " 6100,\n",
       " 7290,\n",
       " 266,\n",
       " 220,\n",
       " 19,\n",
       " 2853,\n",
       " 277,\n",
       " 7290,\n",
       " 13,\n",
       " 395,\n",
       " 282,\n",
       " 2152,\n",
       " 264,\n",
       " 2853,\n",
       " 277,\n",
       " 7290,\n",
       " 220,\n",
       " 3,\n",
       " 16,\n",
       " 20,\n",
       " 11,\n",
       " 3928,\n",
       " 803,\n",
       " 7415,\n",
       " 287,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_pair_encoder.encode(dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding with map\n",
    "Map is a nice way to apply a transfomation to a dataset. It has options for multiprocessing, batching, and it showes your progress.\n",
    "\n",
    "## Batching with BPE\n",
    "If you try and use the batch mode on map it will return a batch with lists, which wont work with the decoder\n",
    "\n",
    "## Multiprocessing with BPE\n",
    "Multiprocessing is tricky in python, because it serializes all of its data into an entirelly new process, and intercommunication between the new process, and the original is difficult and resource intensive because every object must be serialized then unserialized and its asynchronus. \n",
    "Because of this its really tricky to create a new process with the BPE tokenizer, and you have to look into how python multiprocessing works and what kind of spawn time you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1631/1631 [08:41<00:00,  3.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# looks like the encoder cant encode a list of strings into a \"batch\"\n",
    "# lets see if it can encode them all one by one\n",
    "encodedDataset = dataset.map(lambda chunk : {\"tokens\": byte_pair_encoder.encode(str(chunk))})#, remove_columns=\"chunks\")#, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 1631/1631 [02:43<00:00, 10.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# with multiproc\n",
    "encodedDataset = dataset.map(lambda chunk : {\"tokens\": byte_pair_encoder.encode(str(chunk))}, remove_columns=\"chunks\", num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1631"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encodedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It possible to do something like this with batching built in\n",
    "\n",
    "# batch_size = 16\n",
    "# encodedDataset = dataset.map(byte_pair_encoder.encodethingy, batched=True, batch_size=batch_size, num_proc=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Labels to the old dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Everything past this is testing that isnt done yet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "block_size = 256\n",
    "batch_size = 16\n",
    "ix = torch.randint(len(encodedDataset) - block_size, (batch_size,))\n",
    "x = torch.stack([encodedDataset[i:i + block_size] for i in ix])\n",
    "y = torch.stack([encodedDataset[i + 1:i + block_size + 1] for i in ix])\n",
    "# device = data[0].device\n",
    "# x, y = x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, Y = load_batch(data, block_size, batch_size)\n",
    "X, Y = X.to(device), Y.to(device)\n",
    "logits, loss = model(X[i:i + 1], Y[i:i + 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
